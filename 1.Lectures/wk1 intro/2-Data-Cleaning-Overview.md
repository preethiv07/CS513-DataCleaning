# Lecture: Data Cleaning Overview

## âœ… Summary

This lecture introduces the core concept of **data cleaning (aka data wrangling)** and its role in the broader data science lifecycle. It emphasizes that data cleaning is essential â€” though often overlooked â€” and makes up the bulk of the effort in real-world data projects.

Key points discussed include:
- The **difference between data wrangling and analytics**, and why wrangling matters more than it gets credit for.
- Real-world **costs of low-quality data**: monetary loss, confusion, inefficiencies, and even legal risks.
- The **goal of data cleaning**: understanding, assessing, and improving data quality.
- Introduction to key **error types** (quantitative vs qualitative).
- Course themes and tools: regular expressions, OpenRefine, SQL/Datalog, and workflow/provenance tracking using YesWorkflow.

---

## ğŸ“š Details

### ğŸ”· What is Data Wrangling?

**Data wrangling** involves preparing raw data for meaningful analysis. It includes:
- ETL: Extract, Transform, Load
- Data integration
- Cleaning and querying
- Repairing inconsistencies

ğŸ§  **Key insight**: While data analytics is the â€œglamorousâ€ part of data science, **80% of the work is in wrangling**, and itâ€™s what makes reliable analytics possible.

---

### ğŸ”¸ Skills Data Scientists Need

A study of LinkedIn job data (CrowdFlower) shows:
- SQL is the **most in-demand** skill.
- Other database skills (Oracle, PostgreSQL, NoSQL) also highly valued.

ğŸ“Œ Even though this course doesnâ€™t teach SQL directly, you are expected to **think like a database person**.

---

### ğŸ”¸ The High Cost of Dirty Data

Low-quality data leads to both **direct and indirect losses**. Examples include:
- Rework costs, re-entry, misinterpretation
- Lost customers, lost revenue, lawsuits
- System migration, privacy violations, failed operations

Source: *Ge & Helfert, Handbook of Data Quality*

ğŸ’¡ Takeaway: Dirty data is costly even if you choose to ignore or live with it.

> BUNK Cost:A sunk cost in a bad or worthless investment.
---

### ğŸ”¸ What is the Goal of Data Cleaning?

Data cleaning aims to:
- **Understand**: Whatâ€™s wrong with the data?
- **Assess**: What needs fixing?
- **Improve**: Clean it to meet the standards.

This can be remembered via the **three Qs**:
1. **Quality Dimensions** â€“ Accuracy, Timeliness, Completeness, Relevance
2. **Fitness for Use** â€“ Is it good enough to answer our questions?
3. **Queries** â€“ Use SQL/Datalog to profile, validate, and clean data.

---

### ğŸ”¸ Where Does Cleaning Fit in the Lifecycle?

The data lifecycle looks like this:

```text
Gather â†’ Profile â†’ Clean â†’ Analyze
```

However, in reality, this is not a strict pipeline â€” itâ€™s often cyclical:
	â€¢	Data profiling identifies errors
	â€¢	Cleaning standardizes and fixes them
	â€¢	Controlled vocabularies and reference dictionaries help with normalization
	â€¢	Data integration introduces new errors due to source mismatches

### ğŸ”¸ Types of Data Errors

From Abedjan et al.'s taxonomy:

| Type                  | Description                                      | Tools Used                      |
|-----------------------|--------------------------------------------------|----------------------------------|
| **Quantitative Errors** | Outliers or numerical anomalies                  | Statistical or ML-based methods (not in this course) |
| **Qualitative Errors**  | Syntax or schema issues â€” the courseâ€™s main focus | Regex, SQL, Datalog, OpenRefine |

**Subtypes of qualitative errors include:**

- **Syntactic violations**:  
  _e.g., multiple ways to write the same date or place_  
  â¤ Fixable via regex, OpenRefine, or controlled vocabularies.

- **Schema/Integrity Constraint Violations**:  
  _e.g., Foreign key mismatch like PERSON.ZIP not matching ADDRESS.ZIP_  
  â¤ Fixable via SQL or Datalog rules.

- **Duplicates**:  
  _e.g., â€œJoe Doeâ€ appearing twice with different spellings or IDs_  
  â¤ Fixable via matching and de-duplication tools.

---

### ğŸ”¸ Tools and Themes Youâ€™ll Learn

| Theme              | Concepts & Tools Covered                            |
|--------------------|-----------------------------------------------------|
| **Syntax**         | Use of **regular expressions** to identify and transform patterns |
| **OpenRefine**     | GUI tool to clean, transform, and cluster data before DB import |
| **Schema & Semantics** | Use **SQL and Datalog** to define and enforce data rules (ICs) |
| **Synthesis & Provenance** | Use **YesWorkflow** to model scripts as workflows and track data lineage |


### ğŸ”¸ Real Data Example: Issues Even After Cleaning

Even after using OpenRefine or scripts, issues might persist:
	â€¢	Duplicate IDs (e.g., two persons with ID 43)
	â€¢	Format inconsistencies (name order, date formats)
	â€¢	Empty or placeholder values (e.g., 999-999-9999, NULL)
	â€¢	Referential integrity violations (missing ZIP code in ADDRESS table)

ğŸ“Œ Database constraints and SQL/Datalog rules will help us detect and address these.

### ğŸ”¸ Workflow Automation and Provenance
	â€¢	In real-world projects, data cleaning is often done via scripts.
	â€¢	YesWorkflow lets us:
	â€¢	Annotate those scripts
	â€¢	Visualize them as workflows
	â€¢	Track data lineage and dependencies

This is key for reproducibility and auditability in data science pipelines.

### ğŸ§  Closing Thought

Data cleaning isnâ€™t glamorous, but itâ€™s the foundation of every reliable analysis.
