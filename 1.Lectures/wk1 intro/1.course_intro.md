# CS513: Theory and Practice of Data Cleaning


# 🎬 Lecture: Course Introduction

⸻

# 🔍 Why Data Cleaning Matters
	•	Dirty data = Real-world problem
	•	Even after collecting data, it’s often inconsistent or misleading.
	•	Common example: inconsistent date formats (e.g., 02/03/2023 vs 2023-03-02) can break analyses.
	•	XKCD comic illustrates how different formats confuse aggregation tasks.
	•	“Trash in, trash out”
	•	Bad data leads to bad analysis.
	•	Fixing this is essential before doing any machine learning or statistics.

⸻

# 🧰 What is Data Cleaning (aka Data Wrangling)?
	•	Steps involved:
	•	Data extraction
	•	Data integration
	•	Data cleaning
	•	Querying
	•	This is the most time-consuming and underappreciated part of data science:
# 🔸 Data wrangling is ~80% of the work but only gets ~20% of the credit.
	•	Early stages involve data profiling (detecting problems), followed by standardization and normalization.

⸻

# ✅ What is “Good Data”?
	•	Hard to define — depends on context.
	•	“Noise” for one task may be “signal” for another.
	•	Key concept: Fitness for use
	•	Data is clean enough if it suits its intended analytical purpose.

⸻

# 🛠️ Tools & Topics You’ll Learn
	1.	OpenRefine (formerly Google Refine)
	•	Interactive GUI for profiling and cleaning data
	•	Great for non-programmers and quick visual exploration
	2.	Regular Expressions
	•	Helps define patterns to match and transform messy strings
	3.	SQL & Datalog
	•	SQL for querying and checking integrity constraints
	•	Datalog to express logic-based rules for data integrity
	4.	Data Integrity Constraints
	•	E.g., ZIP code in one table must exist in another → checked via queries
	5.	Automatic Data Repair
	•	Cutting-edge research to automatically fix constraint violations
	6.	Workflow Automation & Provenance
	•	Use Python scripts to automate cleaning
	•	Learn how to document cleaning workflows
	•	Understand data provenance — the origin and transformation history of data

# ⭐ At-a-Glance Key Takeaways

| Topic | Key Ideas |
|----|----|
|Data Cleaning Importance|Crucial for meaningful analysis; date formats are a prime issue|
|Cleaning Steps|Extraction → Integration → Cleaning → Querying|
|Fitness for Use|Data quality is task-dependent|
|Tools|OpenRefine, RegEx, SQL, Datalog, Python|
|Concepts|Integrity Constraints, Workflow Modeling, Data Provenance
|Industry Insight|80% effort in cleaning, but only 20% gets noticed|
