# Overview

When confronted with a new dataset, data exploration is often the first step of the data cleaning process. Data exploration and visualization methods are used to identify structures, patterns, and develop a strategy for addressing errors and other quality issues. Data profiling is a more structured process used to identify and quantify key characteristics of datasets and potential data quality concerns. Profiling may include identifying cardinalities, data types, value distributions, correlations,  constraints, and duplicates.

A variety of tools are available for profiling and exploration, both programmatic and interactive. In this week we focus on OpenRefine, a powerful open-source tool for data cleaning. We will work with two different public datasets that can be used to highlight syntactic data quality issues (i.e., the USDA's Farmers' Markets dataset, and the New York Public Library's collection of historic restaurant menus). We will revisit these datasets at the end of the semester in the final project.

# Weekly Readings:

[ACD+16]  Abedjan, Z., Chu, X., Deng, D., Fernandez, R. C., Ilyas, I. F., Ouzzani, M., Papotti, P., Stonebraker, M., & Tang, N. (2016). 
Detecting data errors: Where are we and what needs to be done?
 Proc. VLDB Endow., 9(12), 993–1004.
[AGN15] Abedjan, Z., Golab, L. & Naumann, F. (2015). 
Profiling relational data: a survey
. The VLDB Journal 24, 557–581 (2015).
[PT20] Petrova-Antonova, D. & Tancheva, R. (2020). 
Data Cleaning: A Case Study with OpenRefine and Trifacta Wrangler
. In Quality of Information and Communications Technology. 
Time

This module should take **approximately 7 hours of dedicated time to complete**, with its videos, readings, quiz, and assignments.

# Lessons

The lessons for this module are listed below (with assignments in bold italics):

| Activity | Estimated Time Required |
| ------ | ---- | ---- |
|Lesson 1 Lecture Videos|1 hour|
|Readings|1 hour|
|Week 3 Quiz|1 hour|
|OpenRefine Homework|4 hours|

# Goals and Objectives

Upon successful completion of this module, you will be able to:

Identify key data profiling tasks and understand the trade-offs between interactive and non-interactive methods of profiling.
Create a new OpenRefine project and load tabular data into the system.
Get an overview of a dataset using OpenRefine functions.
Apply common syntactic transformations, such as trimming or collapsing white-spaces.
Use facets and clustering for creating normalized, canonical names.
Improve the data quality of a dataset with OpenRefine.
Document and reapply changes made to the data, using OpenRefine's operation history.
Key Phrases/Concepts

Keep your eyes open for the following key terms or phrases as you interact with the lectures and complete the activities.  

### Common transformations
- Data type conversions
- Facets (text, timeline, scatterplot)
- Clustering
- Normalization of data columns  
- Operation history and provenance 


#### Guiding Questions

Develop your answers to the following guiding questions while watching lectures and working on assignments throughout the module.

- How can OpenRefine be used for data cleaning?
- What are some key functions that OpenRefine offers for cleaning data? 
- What are some (theoretical and practical) limitations of OpenRefine? 
- What kinds of data quality issues can and can't be tackled with OpenRefine?
- How does OpenRefine compare with other tools commonly used for tabular data (e.g., Excel or SQL databases)?